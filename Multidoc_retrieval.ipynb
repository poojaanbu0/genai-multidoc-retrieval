{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojaanbu0/genai-multidoc-retrieval/blob/main/Multidoc_retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b523e0a",
      "metadata": {
        "id": "0b523e0a"
      },
      "source": [
        "#Exp 4: Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f"
      },
      "outputs": [],
      "source": [
        "from helper import get_openai_api_key\n",
        "OPENAI_API_KEY = get_openai_api_key()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221a474-5817-4db2-af46-e029042a75a5",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "3221a474-5817-4db2-af46-e029042a75a5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
      "metadata": {
        "height": 200,
        "tags": [],
        "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0"
      },
      "outputs": [],
      "source": [
        "papers = [\n",
        "    \"docs/metagpt.pdf\",\n",
        "    \"docs/longlora.pdf\",\n",
        "    \"docs/selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
        "outputId": "e4b89a34-133c-4f14-d921-53271247ad3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: docs/metagpt.pdf\n",
            "Getting tools for paper: docs/longlora.pdf\n",
            "Getting tools for paper: docs/selfrag.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff58c52",
      "metadata": {
        "height": 64,
        "id": "bff58c52"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2c6a9f",
      "metadata": {
        "height": 30,
        "id": "2f2c6a9f",
        "outputId": "c0dcf4c3-952b-4f02-a200-abe7f42ec07c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a124a438-5609-402e-8642-69d1088cb9ad",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "a124a438-5609-402e-8642-69d1088cb9ad"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
        "outputId": "e8e780d7-96b5-4c6a-b471-57c4e2f2574c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in the provided context includes the RedPajama dataset, PG19 validation split, book corpus dataset PG19, cleaned Arxiv Math proof-pile dataset, LongBench, LEval, and LongAlpaca-12k.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results across various experiments and comparisons demonstrate the effectiveness of different methods in extending context windows, achieving comparable performance to full fine-tuning while reducing computational costs, improving efficiency, maintaining performance in long context adaptation, and showcasing state-of-the-art performance in handling long-context tasks. The results also highlight the impact of different attention patterns, normalization layers, and embedding layers on model performance, providing insights into the effectiveness of these configurations in various tasks and benchmarks.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA includes the RedPajama dataset, PG19 validation split, book corpus dataset PG19, cleaned Arxiv Math proof-pile dataset, LongBench, LEval, and LongAlpaca-12k. \n",
            "\n",
            "Regarding the evaluation results, they demonstrate the effectiveness of different methods in extending context windows, achieving comparable performance to full fine-tuning while reducing computational costs, improving efficiency, maintaining performance in long context adaptation, and showcasing state-of-the-art performance in handling long-context tasks. The results also highlight the impact of different attention patterns, normalization layers, and embedding layers on model performance, providing insights into the effectiveness of these configurations in various tasks and benchmarks.\n"
          ]
        }
      ],
      "source": [
        "# The agent can then choose the appropriate tools for each steps\n",
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c00137",
      "metadata": {
        "id": "89c00137",
        "outputId": "96cffb72-24f9-484d-8898-7d02d1353e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the tools you leverage to answer the above question.\n",
            "=== LLM Response ===\n",
            "To provide information on the evaluation dataset and evaluation results in LongLoRA, I utilized the following tools:\n",
            "\n",
            "1. **LongLoRA Summary Tool**: This tool was used to extract a summary of the evaluation dataset used in LongLoRA, including details about the datasets involved in the evaluation process.\n",
            "\n",
            "2. **LongLoRA Summary Tool**: I also used this tool to generate a summary of the evaluation results in LongLoRA, which includes insights into the performance of different methods, comparisons, and configurations in various experiments.\n",
            "\n",
            "These tools helped me efficiently gather and present the relevant information about the evaluation dataset and evaluation results in LongLoRA.\n"
          ]
        }
      ],
      "source": [
        "# we can chat/query about the LLM's reasoning process\n",
        "response = agent.chat(\n",
        "    \"Tell me about the tools you leverage to answer the above question.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
        "outputId": "2febcf8e-4496-4ed8-a719-6032b38e080b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: docs/metagpt.pdf\n",
            "Getting tools for paper: docs/longlora.pdf\n",
            "Getting tools for paper: docs/selfrag.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "20154923-873e-4941-9a3a-4926ab5f9b8c"
      },
      "outputs": [],
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "671582f9-70d7-4a8f-b813-58b2a068ca72"
      },
      "outputs": [],
      "source": [
        "# define an \"object\" index over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c3929882-e9dc-46ca-b495-53e3ed60340e"
      },
      "outputs": [],
      "source": [
        "# define the \"retriever\" over the index with specified retrieval method\n",
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03"
      },
      "outputs": [],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and longLora\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
        "outputId": "f4a6ede4-5e60-4eab-c1cc-a030683196f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolMetadata(description='Use ONLY IF you want to get a holistic summary of MetaGPT. Do NOT use if you have specific questions over MetaGPT.', name='summary_tool_longlora', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check for the top 3 tools selected\n",
        "tools[0].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
      "metadata": {
        "height": 268,
        "tags": [],
        "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7"
      },
      "outputs": [],
      "source": [
        "# Define the agentWorker and agentRunner\n",
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
        "outputId": "b694cf01-9f2b-4099-9a66-c9e77854d679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against longLora\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation datasets used in MetaGPT are HumanEval, MBPP, and SoftwareDev.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"evaluation dataset used in longLora\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 validation set from Rae et al., 2020.\n",
            "=== LLM Response ===\n",
            "The evaluation datasets used in MetaGPT are HumanEval, MBPP, and SoftwareDev. On the other hand, the evaluation dataset used in longLora is the PG19 validation set from Rae et al., 2020.\n",
            "assistant: The evaluation datasets used in MetaGPT are HumanEval, MBPP, and SoftwareDev. On the other hand, the evaluation dataset used in longLora is the PG19 validation set from Rae et al., 2020.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against longLora\"\n",
        ")\n",
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}